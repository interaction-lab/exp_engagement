{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import six\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/p7_master_window.csv')\n",
    "df.fillna(value=0, inplace=True)\n",
    "df = df[['op_num_people', 'of_pose_distance', 'of_gaze_distance', 'of_success', 'of_confidence', 'ros_difficulty', 'ros_mistakes_session', 'ros_ts_robot_talked', 'engagement']]\n",
    "\n",
    "len_df = len(df)\n",
    "train_split_ratio = 0.6\n",
    "val_split_ratio = 0.1\n",
    "test_split_ratio = 0.3\n",
    "train_split = int(train_split_ratio * len_df)\n",
    "val_split = int(val_split_ratio * len_df)\n",
    "test_split = int(test_split_ratio * len_df)\n",
    "\n",
    "train_df, val_df, test_df = df.iloc[:train_split,:], df.iloc[train_split:train_split + val_split,:]\\\n",
    "                            ,df.iloc[train_split + val_split:,:]\n",
    "\n",
    "feats = []\n",
    "for i in df.columns:\n",
    "    if i!= 'engagement':\n",
    "        feats.append(i)\n",
    "label = ['engagement']\n",
    "\n",
    "X = df[feats]\n",
    "Y = df[label]\n",
    "X_train = train_df[feats]\n",
    "y_train = train_df[label]#.astype(int)\n",
    "X_test = test_df[feats]\n",
    "y_test = test_df[label]#.astype(int)\n",
    "X_vals = val_df[feats]\n",
    "y_vals = val_df[label]#.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = train_df[train_df['engagement'] == 1][feats]\n",
    "y_pos = train_df[train_df['engagement'] == 1][label].astype(int)\n",
    "X_neg = train_df[train_df['engagement'] == 0][feats]\n",
    "y_neg = train_df[train_df['engagement'] == 0][label].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12075, 8) (12075, 1)\n",
      "(2012, 8) (2012, 1)\n",
      "(6039, 8) (6039, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_vals.shape, y_vals.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    7436\n",
      "0.0    4639\n",
      "Name: engagement, dtype: int64\n",
      "1.0    1109\n",
      "0.0     903\n",
      "Name: engagement, dtype: int64\n",
      "1.0    3484\n",
      "0.0    2555\n",
      "Name: engagement, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.engagement.value_counts())\n",
    "print(y_vals.engagement.value_counts())\n",
    "print(y_test.engagement.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6158178053830228"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratio of positive examples in training\n",
    "y_train.engagement.value_counts()[1] / (y_train.engagement.value_counts()[0]\n",
    "                                       + y_train.engagement.value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "LR = 0.001\n",
    "NUM_STEPS = 1000\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = \" --exp ours\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "flags.DEFINE_float('pos_ratio', 0.6158178053830228, 'Ratio of positive examples in training')\n",
    "flags.DEFINE_integer('nrun', 1, 'Number of runs')\n",
    "flags.DEFINE_integer('ntest', 6039, 'Number of testing examples')\n",
    "flags.DEFINE_integer('ntrain', 12075, 'Number of training examples')\n",
    "flags.DEFINE_integer('nval', 2012, 'Number of validation examples')\n",
    "flags.DEFINE_bool('verbose', True, 'Whether to print training progress')\n",
    "flags.DEFINE_bool('tensorboard', False, 'Whether to save training progress')\n",
    "flags.DEFINE_string('exp', 'ours', 'Which experiment to run')\n",
    "FLAGS = tf.flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = namedtuple('Config', [\n",
    "    'reweight', 'lr', 'num_steps', 'random', 'ratio_weighted', 'nval', 'hard_mining', 'bsize'\n",
    "])\n",
    "\n",
    "exp_repo = dict()\n",
    "\n",
    "def RegisterExp(name):\n",
    "    def _decorator(f):\n",
    "        exp_repo[name] = f\n",
    "        return f\n",
    "\n",
    "    return _decorator\n",
    "\n",
    "@RegisterExp('ours')\n",
    "def ours_config():\n",
    "    return Config(\n",
    "        reweight=True,\n",
    "        num_steps=NUM_STEPS,\n",
    "        lr=LR,\n",
    "        random=False,\n",
    "        ratio_weighted=False,\n",
    "        hard_mining=False,\n",
    "        bsize=100,\n",
    "        nval=FLAGS.nval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "val_set = tf.data.Dataset.from_tensor_slices((X_vals.values, y_vals.values))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
    "\n",
    "train_pos_set = tf.data.Dataset.from_tensor_slices((X_pos.values, y_pos.values))\n",
    "train_neg_set = tf.data.Dataset.from_tensor_slices((X_neg.values, y_neg.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ratio = FLAGS.pos_ratio\n",
    "ntrain = FLAGS.ntrain\n",
    "nval = FLAGS.nval\n",
    "ntest = FLAGS.ntest\n",
    "seed = 42\n",
    "exp_name = 'ours'\n",
    "config = exp_repo['ours']()\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count = len(feats)\n",
    "label_count = 1\n",
    "hidden_layers = feature_count - 1\n",
    "is_training=tf.Variable(True,dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(logits, y):\n",
    "    prediction = tf.cast(tf.sigmoid(logits) > 0.5, tf.float32)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(prediction, y), tf.float32))\n",
    "\n",
    "def get_model(inputs, labels, is_training=True, dtype=tf.float32, w_dict=None, ex_wts=None, reuse=None):\n",
    "    \n",
    "    w_dict = {} \n",
    "    \n",
    "#     def _get_var(name, shape, dtype, initializer):\n",
    "#         key = tf.get_variable_scope().name + '/' + name\n",
    "#         if key in w_dict:\n",
    "#             return w_dict[key]\n",
    "#         else:\n",
    "#             var = tf.get_variable(name, shape, dtype, initializer=initializer)\n",
    "#             w_dict[key] = var\n",
    "#             return var\n",
    "    \n",
    "    with tf.variable_scope('Model', reuse=reuse):\n",
    "        inputs_ = tf.cast(inputs, dtype)\n",
    "        labels = tf.cast(labels, dtype)\n",
    "        act = tf.nn.relu\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        h0 = tf.layers.dense(inputs, hidden_layers, activation=act, kernel_initializer=initializer)\n",
    "        h1 = tf.layers.dense(h0, hidden_layers, activation=act, kernel_initializer=initializer)\n",
    "        h2 = tf.layers.dense(h1, hidden_layers, activation=act, kernel_initializer=initializer)\n",
    "        h3 = tf.layers.dense(h2, hidden_layers, activation=act, kernel_initializer=initializer)\n",
    "        h4 = tf.layers.dense(h3, label_count, activation=None)   \n",
    "\n",
    "        logits = h4\n",
    "\n",
    "        if ex_wts is None:\n",
    "            # Average loss.\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        else:\n",
    "            # Weighted loss.\n",
    "            loss = tf.reduce_sum(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels) * ex_wts)\n",
    "    return w_dict, loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sess, x_, y_, acc_, train_set, test_set):\n",
    "    \n",
    "    train_set_ie = train_set.batch(100)\n",
    "    train_iterator_ie = train_set_ie.make_one_shot_iterator()\n",
    "\n",
    "    test_set_ie = test_set.batch(100)\n",
    "    test_iterator_ie = test_set_ie.make_one_shot_iterator()\n",
    "    \n",
    "    # Calculate final results.\n",
    "    acc_sum = 0.0\n",
    "    acc_test_sum = 0.0\n",
    "    train_bsize = 100\n",
    "    for step in six.moves.xrange(5000 // train_bsize):\n",
    "        x, y = train_iterator_ie.get_next()\n",
    "        acc = sess.run(acc_, feed_dict={x_: x.eval(), y_: y.eval()})\n",
    "        acc_sum += acc\n",
    "\n",
    "    test_bsize = 100\n",
    "    for step in six.moves.xrange(500 // test_bsize):\n",
    "        x_test, y_test = test_iterator_ie.get_next()\n",
    "        acc = sess.run(acc_, feed_dict={x_: x_test.eval(), y_: y_test.eval()})\n",
    "        acc_test_sum += acc\n",
    "\n",
    "    train_acc = acc_sum / float(5000 // train_bsize)\n",
    "    test_acc = acc_test_sum / float(500 // test_bsize)\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_autodiff(inp_a,\n",
    "                      label_a,\n",
    "                      inp_b,\n",
    "                      label_b,\n",
    "                      bsize_a,\n",
    "                      bsize_b,\n",
    "                      eps=0.0,\n",
    "                      gate_gradients=1):\n",
    "    \"\"\"Reweight examples using automatic differentiation.\n",
    "    :param inp_a:             [Tensor]    Inputs for the noisy pass.\n",
    "    :param label_a:           [Tensor]    Labels for the noisy pass.\n",
    "    :param inp_b:             [Tensor]    Inputs for the clean pass.\n",
    "    :param label_b:           [Tensor]    Labels for the clean pass.\n",
    "    :param bsize_a:           [int]       Batch size for the noisy pass.\n",
    "    :param bsize_b:           [int]       Batch size for the clean pass.\n",
    "    :param eps:               [float]     Minimum example weights, default 0.0.\n",
    "    :param gate_gradients:    [int]       Tensorflow gate gradients, reduce concurrency.\n",
    "    \"\"\"\n",
    "    ex_wts_a = tf.zeros([bsize_a], dtype=tf.float32)\n",
    "    ex_wts_b = tf.ones([bsize_b], dtype=tf.float32) / float(bsize_b)\n",
    "    \n",
    "    w_dict, loss_a, logits_a = get_model(\n",
    "        inp_a, label_a, ex_wts=ex_wts_a, is_training=True, reuse=True)\n",
    "    \n",
    "    var_names = w_dict.keys()\n",
    "    var_list = [w_dict[kk] for kk in var_names]\n",
    "    \n",
    "    grads = tf.gradients(loss_a, var_list, gate_gradients=gate_gradients)\n",
    "\n",
    "    var_list_new = [vv - gg for gg, vv in zip(grads, var_list)]\n",
    "    w_dict_new = dict(zip(var_names, var_list_new))\n",
    "    _, loss_b, logits_b = get_model(\n",
    "        inp_b, label_b, ex_wts=ex_wts_b, is_training=True, reuse=True, w_dict=w_dict_new)\n",
    "    \n",
    "    grads_ex_wts = tf.gradients(loss_b, [ex_wts_a], gate_gradients=gate_gradients)[0]\n",
    "    ex_weight = -grads_ex_wts\n",
    "    ex_weight_plus = tf.maximum(ex_weight, eps)\n",
    "    ex_weight_sum = tf.reduce_sum(ex_weight_plus)\n",
    "    ex_weight_sum += tf.to_float(tf.equal(ex_weight_sum, 0.0))\n",
    "    ex_weight_norm = ex_weight_plus / ex_weight_sum\n",
    "    return ex_weight_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_random(bsize, eps=0.0):\n",
    "    \"\"\"Reweight examples using random numbers.\n",
    "    \n",
    "    :param bsize:             [int]       Batch size.\n",
    "    :param eps:               [float]     Minimum example weights, default 0.0.\n",
    "    \"\"\"\n",
    "    ex_weight = tf.random_normal([bsize], mean=0.0, stddev=1.0)\n",
    "    ex_weight_plus = tf.maximum(ex_weight, eps)\n",
    "    ex_weight_sum = tf.reduce_sum(ex_weight_plus)\n",
    "    ex_weight_sum += tf.to_float(tf.equal(ex_weight_sum, 0.0))\n",
    "    ex_weight_norm = ex_weight_plus / ex_weight_sum\n",
    "    return ex_weight_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = config.bsize\n",
    "num_steps = config.num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bala/anaconda2/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:408: UserWarning: An unusually high number of `Iterator.get_next()` calls was detected. This often indicates that `Iterator.get_next()` is being called inside a training loop, which will cause gradual slowdown and eventual resource exhaustion. If this is the case, restructure your code to call `next_element = iterator.get_next()` once outside the loop, and use `next_element` as the input to some computation that is invoked inside the loop.\n",
      "  warnings.warn(GET_NEXT_CALL_WARNING_MESSAGE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 Loss 121.59438 Train acc 0.6020000010728837 Test acc 0.5060000002384186\n",
      "Step 200 Loss 89.85155 Train acc 0.39799999933689834 Test acc 0.4939999938011169\n",
      "Step 300 Loss 64.72139 Train acc 0.6020000010728837 Test acc 0.5060000002384186\n",
      "Step 400 Loss 133.56561 Train acc 0.6020000010728837 Test acc 0.5060000002384186\n",
      "Step 500 Loss 97.673965 Train acc 0.39799999933689834 Test acc 0.4939999938011169\n",
      "Step 600 Loss 48.5645 Train acc 0.6020000010728837 Test acc 0.5060000002384186\n",
      "Step 700 Loss 160.89932 Train acc 0.6020000010728837 Test acc 0.5060000002384186\n",
      "Step 800 Loss 73.17954 Train acc 0.39799999933689834 Test acc 0.4939999938011169\n",
      "Step 900 Loss 67.498695 Train acc 0.6020000010728837 Test acc 0.5060000002384186\n",
      "Step 1000 Loss 52.861008 Train acc 0.6020000010728837 Test acc 0.5060000002384186\n",
      "Final Train acc 0.6020000010728837 Test acc 0.5060000002384186\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    train_set_i = train_set.repeat(num_steps).batch(bsize)\n",
    "    train_iterator = train_set_i.make_one_shot_iterator()\n",
    "    val_set_i = val_set.repeat(num_steps).batch(bsize)\n",
    "    val_iterator = val_set_i.make_one_shot_iterator()\n",
    "    \n",
    "    config = exp_repo['ours']()\n",
    "    bsize = config.bsize\n",
    "    \n",
    "    x_ = tf.placeholder(tf.float32,[None,feature_count], name='x')\n",
    "    y_ = tf.placeholder(tf.float32, [None,label_count], name='y')\n",
    "    \n",
    "    x_val_ = tf.placeholder(tf.float32, [None, feature_count], name='x_val')\n",
    "    y_val_ = tf.placeholder(tf.float32, [None,label_count], name='y_val')\n",
    "    ex_wts_ = tf.placeholder(tf.float32, [None], name='ex_wts')\n",
    "    lr_ = tf.placeholder(tf.float32, [], name='lr')\n",
    "\n",
    "    # Build training model.\n",
    "    with tf.name_scope('Train'):\n",
    "        _, loss_c, logits_c = get_model(\n",
    "            x_, y_, is_training=True, dtype=tf.float32, w_dict=None, ex_wts=ex_wts_, reuse=None)\n",
    "        train_op = tf.train.MomentumOptimizer(config.lr, 0.9).minimize(loss_c)\n",
    "        \n",
    "    # Build evaluation model.\n",
    "    with tf.name_scope('Val'):\n",
    "        _, loss_eval, logits_eval = get_model(\n",
    "            x_,\n",
    "            y_,\n",
    "            is_training=False,\n",
    "            dtype=tf.float32,\n",
    "            w_dict=None,\n",
    "            ex_wts=ex_wts_,\n",
    "            reuse=True)\n",
    "        acc_ = get_acc(logits_eval, y_)\n",
    "    \n",
    "    # Build reweighting model.\n",
    "    if config.reweight:\n",
    "        ex_weights_ = reweight_random(bsize)\n",
    "#         ex_weights_ = reweight_autodiff(\n",
    "#             x_,\n",
    "#             y_,\n",
    "#             x_val_,\n",
    "#             y_val_,\n",
    "#             bsize,\n",
    "#             min(bsize, nval),\n",
    "#             eps=0.0,\n",
    "#             gate_gradients=1)    \n",
    "        \n",
    "    exp_logger = None\n",
    "\n",
    "    lr = config.lr\n",
    "    num_steps = config.num_steps\n",
    "\n",
    "    acc_sum = 0.0\n",
    "    acc_test_sum = 0.0\n",
    "    loss_sum = 0.0\n",
    "    count = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in six.moves.xrange(num_steps):\n",
    "        x, y = train_iterator.get_next()\n",
    "        x_val, y_val = val_iterator.get_next()\n",
    "\n",
    "        # Use 50% learning rate for the second half of training.\n",
    "        if step > num_steps // 2:\n",
    "            lr = config.lr / 2.0\n",
    "        else:\n",
    "            lr = config.lr\n",
    "\n",
    "        ex_weights = sess.run(\n",
    "            ex_weights_, feed_dict={x_: x.eval(),\n",
    "                                    y_: y.eval(),\n",
    "                                    x_val_: x_val.eval(),\n",
    "                                    y_val_: y_val.eval()})\n",
    "        loss, acc, _ = sess.run(\n",
    "            [loss_c, acc_, train_op],\n",
    "            feed_dict={\n",
    "                x_: x.eval(),\n",
    "                y_: y.eval(),\n",
    "                x_val_: x_val.eval(),\n",
    "                y_val_: y_val.eval(),\n",
    "                ex_wts_: ex_weights,\n",
    "                lr_: lr\n",
    "            })\n",
    "        if (step + 1) % 100 == 0:\n",
    "            train_acc, test_acc = evaluate(sess, x_, y_, acc_, train_set, test_set)\n",
    "            if verbose:\n",
    "                print('Step', step + 1, 'Loss', loss, 'Train acc', train_acc, 'Test acc',\n",
    "                      test_acc)\n",
    "            if FLAGS.tensorboard:\n",
    "                exp_logger.log(step + 1, 'train acc', train_acc)\n",
    "                exp_logger.log(step + 1, 'test acc', test_acc)\n",
    "                exp_logger.flush()\n",
    "            acc_sum = 0.0\n",
    "            loss_sum = 0.0\n",
    "            acc_test_sum = 0.0\n",
    "            count = 0\n",
    "\n",
    "    # Final evaluation.\n",
    "    train_acc, test_acc = evaluate(sess, x_, y_, acc_, train_set, test_set)\n",
    "    if verbose:\n",
    "        print('Final', 'Train acc', train_acc, 'Test acc', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
