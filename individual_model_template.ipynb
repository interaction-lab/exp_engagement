{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template Notebook for Indvidualized Participant Engagement Classifcation NN Model\n",
    "\n",
    "# ToDo/Goals: https://docs.google.com/document/d/1_dTnbdRoBV0A7xCxVEyLBdckrlsUnMnRFbbRn39tiXk/edit?usp=sharing\n",
    "# Data: https://drive.google.com/drive/folders/19aJUAlkTMz7PcZE1q4hogFkjVtwYGcMT\n",
    "# Upload code to help-seeking github: https://github.com/interaction-lab/help_seeking\n",
    "# Record model results here: https://docs.google.com/spreadsheets/d/16ye54fSSEuAuDL_j56UIeDB-rIIrxq_kPbtyPRQOrVI/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import *\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Specifiy Data Path Based on Participant/Your Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify Data Path based on participant/your computer\n",
    "# USE data from FS4 folder\n",
    "\n",
    "file = '../../../Google Drive File Stream/My Drive/USC Expeditions Year 5/Analysis/Help-Seeking/Data/FS4/p7_data_FS4.csv'\n",
    "data = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Analysis\n",
    "\n",
    "print(data['engagement'].value_counts())\n",
    "print()\n",
    "\n",
    "print(1,data['engagement'].value_counts()[1]/sum(data['engagement'].value_counts()))\n",
    "print(0,data['engagement'].value_counts()[0]/sum(data['engagement'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split (for chosen Feature Set)\n",
    "\n",
    "#### TODO: Specify which train-test split you want\n",
    "\n",
    "1. Random Shuffle Split: 70-30, 50-50, 30-70, 10-90\n",
    "2. Chronological Split: 70-30, 50-50, 30-70\n",
    "3. Session Split: 70-30, 50-50, 30-70, 10-90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session Split: Train on Percentage of Data From Each Session, Sorted Chronologically \n",
    "\n",
    "def session_split(data, size):\n",
    "    sessions = data['session_num'].unique()\n",
    "    \n",
    "    first = True\n",
    "        \n",
    "    for s in sessions:\n",
    "        curr = data.loc[data['session_num'] == s]\n",
    "        curr = curr.sort_values(['timestamp'], ascending=[True])\n",
    "\n",
    "        y = curr['engagement']\n",
    "        X = curr.drop(columns=['timestamp', 'session_num', 'engagement'])\n",
    "\n",
    "        if first:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size, shuffle=True)\n",
    "            first = False\n",
    "            \n",
    "        else:\n",
    "            CX_train, CX_test, Cy_train, Cy_test = train_test_split(X, y, test_size=size, shuffle=True)\n",
    "                        \n",
    "            X_train = X_train.append(CX_train)\n",
    "            X_test = X_test.append(CX_test)\n",
    "            y_train = y_train.append(Cy_train)\n",
    "            y_test = y_test.append(Cy_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data (don't need for Random Shuffle, but ok to leave it)\n",
    "\n",
    "data = data.sort_values(['session_num', 'timestamp'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment train-test split to run below \n",
    "\n",
    "y = data['engagement']\n",
    "X = data.drop(columns=['timestamp', 'session_num', 'engagement'])\n",
    "\n",
    "# Choose one of these \n",
    "\n",
    "\n",
    "# 1. Random Shuffle Split 50-50\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "### (Extra)\n",
    "# Random Shuffle Split 70-30\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Random Shuffle Split 30-70\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "\n",
    "# Random Shuffle Split 10-90\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "### \n",
    "\n",
    "#############################\n",
    "\n",
    "# 2. Chronological Split 70-30\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# 3. Chronological Split 50-50\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "\n",
    "# 4. Chronological Split 30-70\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, shuffle=False)\n",
    "\n",
    "############################# \n",
    "\n",
    "# 5. Session Split 70-30\n",
    "# X_train, X_test, y_train, y_test = session_split(data, 0.3)\n",
    "\n",
    "# 6. Session Split 50-50\n",
    "# X_train, X_test, y_train, y_test = session_split(data, 0.5)\n",
    "\n",
    "# 7. Session Split 30-70\n",
    "# X_train, X_test, y_train, y_test = session_split(data, 0.7)\n",
    "\n",
    "# 8. Session Split 10-90\n",
    "# X_train, X_test, y_train, y_test = session_split(data, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort X,y Train dataframes together after split \n",
    "\n",
    "X_train, bogus_test, y_train, bogus_test = train_test_split(X_train, y_train, test_size=0.0, shuffle=True)\n",
    "X_train, bogus_test, y_train, bogus_test = train_test_split(X_train, y_train, test_size=0.0, shuffle=True)\n",
    "X_train, bogus_test, y_train, bogus_test = train_test_split(X_train, y_train, test_size=0.0, shuffle=True)\n",
    "X_train, bogus_test, y_train, bogus_test = train_test_split(X_train, y_train, test_size=0.0, shuffle=True)\n",
    "X_train, bogus_test, y_train, bogus_test = train_test_split(X_train, y_train, test_size=0.0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: May need to adjust following if poor accurracy\n",
    "1. Vary num_epochs, batch size\n",
    "2. Vary between 2, 3, 4 hidden layers\n",
    "3. Vary amount of nodes in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10       # if bad, try 1 - 20\n",
    "size_batch = 32      # if bad, try 16, 32, 64, 128, 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN Model for FS4 (all OF/OP)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#model.add(Dense(units=256, input_dim=(X_train.shape[1]), kernel_initializer='glorot_normal'))\n",
    "model.add(Dense(units=64, input_dim=(X_train.shape[1]), kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#model.add(Dense(units=96, kernel_initializer='glorot_normal'))\n",
    "model.add(Dense(units=32, kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#model.add(Dense(units=64, kernel_initializer='glorot_normal'))\n",
    "model.add(Dense(units=16, kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#model.add(Dense(units=24, kernel_initializer='glorot_normal'))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(units=1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',  \n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use validation split to monitor training, don't use for final recorded accurracy \n",
    "# model.fit(X_train, y_train, epochs=num_epochs, batch_size=size_batch, verbose=1, validation_split=0.1)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=num_epochs, batch_size=size_batch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, y_test)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict_proba(X_test)\n",
    "pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: RECORD ACCURACY IN MODEL RESULTS SPREADSHEET\n",
    "Try to get accurracy, AUC above 90%!!!\n",
    "See results for P8 for guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "except:\n",
    "    print(\"AUC undefinied, only 1 class in test data\")\n",
    "    \n",
    "print(\"Accuracy:\",accuracy_score(y_test, pred))\n",
    "print(metrics.confusion_matrix(y_test, pred))\n",
    "print(metrics.classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:caispp]",
   "language": "python",
   "name": "conda-env-caispp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
