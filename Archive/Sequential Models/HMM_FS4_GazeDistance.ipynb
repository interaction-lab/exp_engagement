{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HMM for FS4_new (Handpicked OF/OP + P + GazeDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import *\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file3 = '../../../../Google Drive File Stream/My Drive/USC Expeditions Year 5/Analysis/Help-Seeking/Data/FS4_new/p3_data_FS4new.csv'\n",
    "file5 = '../../../../Google Drive File Stream/My Drive/USC Expeditions Year 5/Analysis/Help-Seeking/Data/FS4_new/p5_data_FS4new.csv'\n",
    "file7 = '../../../../Google Drive File Stream/My Drive/USC Expeditions Year 5/Analysis/Help-Seeking/Data/FS4_new/p7_data_FS4new.csv'\n",
    "file8 = '../../../../Google Drive File Stream/My Drive/USC Expeditions Year 5/Analysis/Help-Seeking/Data/FS4_new/p8_data_FS4new.csv'\n",
    "file9 = '../../../../Google Drive File Stream/My Drive/USC Expeditions Year 5/Analysis/Help-Seeking/Data/FS4_new/p9_data_FS4new.csv'\n",
    "file10 = '../../../../Google Drive File Stream/My Drive/USC Expeditions Year 5/Analysis/Help-Seeking/Data/FS4_new/p10_data_FS4new.csv'\n",
    "\n",
    "data3 = pd.read_csv(file3)\n",
    "data5 = pd.read_csv(file5)\n",
    "data7 = pd.read_csv(file7)\n",
    "data8 = pd.read_csv(file8)\n",
    "data9 = pd.read_csv(file9)\n",
    "data10 = pd.read_csv(file10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: keep timestamp and session_num until right before running the model in all feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Train Test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on Earlier Sessions and Test on Later Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Set 4_new is used for each participant in set\n",
    "FS = data10\n",
    "FS = FS.sort_values(['session_num', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_threshold(FS, size):\n",
    "    sessions = FS['session_num'].unique()\n",
    "    thresh = size*(len(sessions))\n",
    "    print(sessions)\n",
    "    thresh = round(thresh,0)\n",
    "    return sessions[int(thresh) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def session_split(sess_threshold):\n",
    "    print(sess_threshold)\n",
    "    train = FS[(FS['session_num'] <= sess_threshold)] \n",
    "    test = FS[(FS['session_num'] > sess_threshold)]\n",
    "\n",
    "    # drop columns needed for split\n",
    "    train2 = train.drop(columns=['timestamp'])\n",
    "    test2 = test.drop(columns=['timestamp'])\n",
    "\n",
    "    X_train2 = train2.drop(columns=['engagement'])\n",
    "    y_train2 = train2['engagement']\n",
    "\n",
    "    X_test2 = test2.drop(columns=['engagement'])\n",
    "    y_test2 = test2['engagement']\n",
    "    \n",
    "    return X_train2, y_train2, X_test2, y_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.1  2.2  3.1  3.2  4.2  4.3  4.4  5.1  5.2  7.1  7.2  8.   9. ]\n",
      "3.2\n"
     ]
    }
   ],
   "source": [
    "# choose train-test split\n",
    "#X_train, y_train, X_test, y_test = session_split(find_threshold(FS,0.7))\n",
    "#X_train, y_train, X_test, y_test = session_split(find_threshold(FS,0.5))\n",
    "X_train, y_train, X_test, y_test = session_split(find_threshold(FS,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make HMM train-test inputs\n",
    "lengths_train = X_train['session_num'].value_counts(sort=False).tolist()\n",
    "X_train = X_train.drop(columns=['session_num'])\n",
    "lengths_test = X_test['session_num'].value_counts(sort=False).tolist()\n",
    "X_test = X_test.drop(columns=['session_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement the HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hmmlearn import hmm\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "# 3 questions from HMM:\n",
    "# (1) given model, what is likelihood of sequence S happening?\n",
    "# (2) given model and sequence S, what is optimal hidden state sequence?\n",
    "# (3) given sequence S and # of hidden states, what is optimal model to maximize probability of S?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.30283795159\n",
      "1 0.453608424525\n",
      "2 0.595076531927\n",
      "3 0.697462678875\n",
      "4 0.698072529247\n",
      "5 0.301575303637\n",
      "6 0.546391575475\n",
      "7 0.404923468073\n",
      "8 0.595076531927\n",
      "9 0.613372043085\n"
     ]
    }
   ],
   "source": [
    "# HMM tends to get stuck in local minimum. For best results, run several times and pick best\n",
    "best_model = GaussianHMM(n_components=2, n_iter=5, algorithm = 'viterbi').fit(X_train, lengths_train)\n",
    "best_pred = best_model.predict(X_train, lengths_train)\n",
    "best = accuracy_score(y_train, best_pred)\n",
    "num = 0\n",
    "\n",
    "for i in range(0,10):\n",
    "    new_model = GaussianHMM(n_components=2, n_iter=5, algorithm = 'viterbi', init_params=\"mcs\")\n",
    "    \n",
    "    # randomly initialize probability matrix \n",
    "    a = random.uniform(0, 1)\n",
    "    b = random.uniform(0, 1)\n",
    "    new_model.transmat_ = np.array([[a, 1-a], [b, 1-b]])\n",
    "    new_model = new_model.fit(X_train, lengths_train)\n",
    "    \n",
    "    # evaluate this model and see if better than best so far \n",
    "    new_pred = new_model.predict(X_train, lengths_train)\n",
    "    new_acc = accuracy_score(y_train, new_pred)\n",
    "    print(i, new_acc)\n",
    "    \n",
    "    if (new_acc > best):\n",
    "        best = new_acc\n",
    "        best_model = new_model\n",
    "        num = i\n",
    "         \n",
    "hmm_model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70-30 Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3 FS4_new 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([1484808.2228910511, 1484808.2228911791], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p3 = hmm_model\n",
    "print(model_p3.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P3 HMM: 0.71677864875\n",
      "AUC: 0.7386119225\n",
      "[[ 7173  2118]\n",
      " [13057 31232]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.35      0.77      0.49      9291\n",
      "        1.0       0.94      0.71      0.80     44289\n",
      "\n",
      "avg / total       0.84      0.72      0.75     53580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P3 HMM\n",
    "FS4new_pred3 = model_p3.predict(X_test, lengths_test)\n",
    "scores = model_p3.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P3 HMM:\",accuracy_score(y_test, FS4new_pred3))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred3))\n",
    "print(metrics.classification_report(y_test, FS4new_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5 FS4_new 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([12282441.888420505, 12282441.888408747], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p5 = hmm_model\n",
    "print(model_p5.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P5 HMM: 0.700621308583\n",
      "AUC: 0.695488714772\n",
      "[[ 6249  2839]\n",
      " [12773 30287]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.69      0.44      9088\n",
      "          1       0.91      0.70      0.80     43060\n",
      "\n",
      "avg / total       0.81      0.70      0.73     52148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P5 HMM\n",
    "FS4new_pred5 = model_p5.predict(X_test, lengths_test)\n",
    "scores = model_p5.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P5 HMM:\",accuracy_score(y_test, FS4new_pred5))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred5))\n",
    "print(metrics.classification_report(y_test, FS4new_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P7 FS4_new 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([8390809.2739644442, 8390809.273962304], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p7 = hmm_model\n",
    "print(model_p7.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P7 HMM: 0.833556401819\n",
      "AUC: 0.811524241634\n",
      "[[75036  1217]\n",
      " [21292 37690]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.98      0.87     76253\n",
      "        1.0       0.97      0.64      0.77     58982\n",
      "\n",
      "avg / total       0.86      0.83      0.83    135235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P7 HMM\n",
    "FS4new_pred7 = model_p7.predict(X_test, lengths_test)\n",
    "scores = model_p7.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P7 HMM:\",accuracy_score(y_test, FS4new_pred7))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred7))\n",
    "print(metrics.classification_report(y_test, FS4new_pred7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P8 FS4_new 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([2744713.4677161686, 2773171.3350919969], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p8 = hmm_model\n",
    "print(model_p8.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P8 HMM: 0.334839949758\n",
      "AUC: 0.376723164335\n",
      "[[24198 40379]\n",
      " [29524 10991]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.45      0.37      0.41     64577\n",
      "        1.0       0.21      0.27      0.24     40515\n",
      "\n",
      "avg / total       0.36      0.33      0.34    105092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P8 HMM\n",
    "FS4new_pred8 = model_p8.predict(X_test, lengths_test)\n",
    "scores = model_p8.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P8 HMM:\",accuracy_score(y_test, FS4new_pred8))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred8))\n",
    "print(metrics.classification_report(y_test, FS4new_pred8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P9 FS4_new 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([5919137.5161923338, 8910856.4610294867], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p9 = hmm_model\n",
    "print(model_p9.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P9 HMM: 0.481946787658\n",
      "AUC: 0.551341538457\n",
      "[[35052 72188]\n",
      " [22695 53218]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.33      0.42    107240\n",
      "          1       0.42      0.70      0.53     75913\n",
      "\n",
      "avg / total       0.53      0.48      0.47    183153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P9 HMM\n",
    "FS4new_pred9 = model_p9.predict(X_test, lengths_test)\n",
    "scores = model_p9.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P9 HMM:\",accuracy_score(y_test, FS4new_pred9))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred9))\n",
    "print(metrics.classification_report(y_test, FS4new_pred9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P10 FS4_new 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([5922755.8721404951, 5923405.8099162905], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p10 = hmm_model\n",
    "print(model_p10.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P10 HMM: 0.343454908759\n",
      "AUC: 0.37343898173\n",
      "[[ 91832 236836]\n",
      " [ 41136  53582]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.28      0.40    328668\n",
      "          1       0.18      0.57      0.28     94718\n",
      "\n",
      "avg / total       0.58      0.34      0.37    423386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P10 HMM\n",
    "FS4new_pred10 = model_p10.predict(X_test, lengths_test)\n",
    "scores = model_p10.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P10 HMM:\",accuracy_score(y_test, FS4new_pred10))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred10))\n",
    "print(metrics.classification_report(y_test, FS4new_pred10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50-50 Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3 FS4_new 50-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([1145793.8861379013, 1145793.8861376292], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p3 = hmm_model\n",
    "print(model_p3.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P3 HMM: 0.716374911181\n",
      "AUC: 0.727538936806\n",
      "[[10241  3508]\n",
      " [18446 45210]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.36      0.74      0.48     13749\n",
      "        1.0       0.93      0.71      0.80     63656\n",
      "\n",
      "avg / total       0.83      0.72      0.75     77405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P3 HMM\n",
    "FS4new_pred3 = model_p3.predict(X_test, lengths_test)\n",
    "scores = model_p3.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P3 HMM:\",accuracy_score(y_test, FS4new_pred3))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred3))\n",
    "print(metrics.classification_report(y_test, FS4new_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5 FS4_new 50-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([11188393.266187027, 11188393.266199561], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p5 = hmm_model\n",
    "print(model_p5.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P5 HMM: 0.776544927075\n",
      "AUC: 0.696560032543\n",
      "[[13257 10017]\n",
      " [18097 84444]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.57      0.49     23274\n",
      "          1       0.89      0.82      0.86    102541\n",
      "\n",
      "avg / total       0.81      0.78      0.79    125815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P5 HMM\n",
    "FS4new_pred5 = model_p5.predict(X_test, lengths_test)\n",
    "scores = model_p5.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P5 HMM:\",accuracy_score(y_test, FS4new_pred5))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred5))\n",
    "print(metrics.classification_report(y_test, FS4new_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P7 FS4_new 50-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([8174151.6034516497, 8174151.6034622965], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p7 = hmm_model\n",
    "print(model_p7.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P7 HMM: 0.779357201108\n",
      "AUC: 0.790773740645\n",
      "[[83249  3195]\n",
      " [37659 61056]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.96      0.80     86444\n",
      "        1.0       0.95      0.62      0.75     98715\n",
      "\n",
      "avg / total       0.83      0.78      0.77    185159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P7 HMM\n",
    "FS4new_pred7 = model_p7.predict(X_test, lengths_test)\n",
    "scores = model_p7.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P7 HMM:\",accuracy_score(y_test, FS4new_pred7))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred7))\n",
    "print(metrics.classification_report(y_test, FS4new_pred7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P8 FS4_new 50-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([2005965.8121410313, 2018221.8680601474], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p8 = hmm_model\n",
    "print(model_p8.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P8 HMM: 0.440703663704\n",
      "AUC: 0.520482743825\n",
      "[[12088 73063]\n",
      " [ 7724 51569]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.61      0.14      0.23     85151\n",
      "        1.0       0.41      0.87      0.56     59293\n",
      "\n",
      "avg / total       0.53      0.44      0.37    144444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P8 HMM\n",
    "FS4new_pred8 = model_p8.predict(X_test, lengths_test)\n",
    "scores = model_p8.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P8 HMM:\",accuracy_score(y_test, FS4new_pred8))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred8))\n",
    "print(metrics.classification_report(y_test, FS4new_pred8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P9 FS4_new 50-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([6929385.1297414964, 10065307.728199605], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p9 = hmm_model\n",
    "print(model_p9.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P9 HMM: 0.508636900118\n",
      "AUC: 0.575512449616\n",
      "[[41652 74600]\n",
      " [23651 60053]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.36      0.46    116252\n",
      "          1       0.45      0.72      0.55     83704\n",
      "\n",
      "avg / total       0.56      0.51      0.50    199956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P9 HMM\n",
    "FS4new_pred9 = model_p9.predict(X_test, lengths_test)\n",
    "scores = model_p9.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P9 HMM:\",accuracy_score(y_test, FS4new_pred9))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred9))\n",
    "print(metrics.classification_report(y_test, FS4new_pred9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P10 FS4_new 50-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([7048466.6991112325, 7503006.7531883093], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p10 = hmm_model\n",
    "print(model_p10.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P10 HMM: 0.717349829658\n",
      "AUC: 0.700121683563\n",
      "[[269950  82190]\n",
      " [ 75444 130116]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.77      0.77    352140\n",
      "          1       0.61      0.63      0.62    205560\n",
      "\n",
      "avg / total       0.72      0.72      0.72    557700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P10 HMM\n",
    "FS4new_pred10 = model_p10.predict(X_test, lengths_test)\n",
    "scores = model_p10.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P10 HMM:\",accuracy_score(y_test, FS4new_pred10))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred10))\n",
    "print(metrics.classification_report(y_test, FS4new_pred10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30-70 Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3 FS4_new 30-70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([621787.28552293032, 718104.01118034741], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p3 = hmm_model\n",
    "print(model_p3.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P3 HMM: 0.747819229253\n",
      "AUC: 0.743014393699\n",
      "[[12463  4473]\n",
      " [21748 65293]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.36      0.74      0.49     16936\n",
      "        1.0       0.94      0.75      0.83     87041\n",
      "\n",
      "avg / total       0.84      0.75      0.78    103977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P3 HMM\n",
    "FS4new_pred3 = model_p3.predict(X_test, lengths_test)\n",
    "scores = model_p3.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P3 HMM:\",accuracy_score(y_test, FS4new_pred3))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred3))\n",
    "print(metrics.classification_report(y_test, FS4new_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5 FS4_new 30-70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([2148063.9927106481, 2275317.3949539112], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p5 = hmm_model\n",
    "print(model_p5.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P5 HMM: 0.751096189021\n",
      "AUC: 0.785596487911\n",
      "[[ 85209  13803]\n",
      " [ 55735 124630]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.86      0.71     99012\n",
      "          1       0.90      0.69      0.78    180365\n",
      "\n",
      "avg / total       0.80      0.75      0.76    279377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P5 HMM\n",
    "FS4new_pred5 = model_p5.predict(X_test, lengths_test)\n",
    "scores = model_p5.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P5 HMM:\",accuracy_score(y_test, FS4new_pred5))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred5))\n",
    "print(metrics.classification_report(y_test, FS4new_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P7 FS4_new 30-70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([5481868.1197526753, 5481868.1197468825], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p7 = hmm_model\n",
    "print(model_p7.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P7 HMM: 0.745172475752\n",
      "AUC: 0.762336701353\n",
      "[[95689 14278]\n",
      " [52378 99228]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.65      0.87      0.74    109967\n",
      "        1.0       0.87      0.65      0.75    151606\n",
      "\n",
      "avg / total       0.78      0.75      0.75    261573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P7 HMM\n",
    "FS4new_pred7 = model_p7.predict(X_test, lengths_test)\n",
    "scores = model_p7.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P7 HMM:\",accuracy_score(y_test, FS4new_pred7))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred7))\n",
    "print(metrics.classification_report(y_test, FS4new_pred7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P8 FS4_new 30-70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([1887568.3394431826, 1908349.6150034701], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p8 = hmm_model\n",
    "print(model_p8.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P8 HMM: 0.486877869247\n",
      "AUC: 0.560463683463\n",
      "[[20536 70664]\n",
      " [ 8247 54339]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      0.23      0.34     91200\n",
      "        1.0       0.43      0.87      0.58     62586\n",
      "\n",
      "avg / total       0.60      0.49      0.44    153786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P8 HMM\n",
    "FS4new_pred8 = model_p8.predict(X_test, lengths_test)\n",
    "scores = model_p8.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P8 HMM:\",accuracy_score(y_test, FS4new_pred8))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred8))\n",
    "print(metrics.classification_report(y_test, FS4new_pred8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P9 FS4_new 30-70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([3786942.3555388898, 4094171.5969754853], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p9 = hmm_model\n",
    "print(model_p9.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P9 HMM: 0.49095137572\n",
      "AUC: 0.486581194268\n",
      "[[81946 75034]\n",
      " [67240 55270]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.52      0.54    156980\n",
      "          1       0.42      0.45      0.44    122510\n",
      "\n",
      "avg / total       0.49      0.49      0.49    279490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P9 HMM\n",
    "FS4new_pred9 = model_p9.predict(X_test, lengths_test)\n",
    "scores = model_p9.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P9 HMM:\",accuracy_score(y_test, FS4new_pred9))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred9))\n",
    "print(metrics.classification_report(y_test, FS4new_pred9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P10 FS4_new 30-70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([5581996.1340877358, 6006488.4181838408], maxlen=2)\n"
     ]
    }
   ],
   "source": [
    "# model takes array of dimension (n_samples, n_features)\n",
    "model_p10 = hmm_model\n",
    "print(model_p10.monitor_.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FS4_new P10 HMM: 0.498192818294\n",
      "AUC: 0.480990905016\n",
      "[[185962 179566]\n",
      " [140592 131890]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.51      0.54    365528\n",
      "          1       0.42      0.48      0.45    272482\n",
      "\n",
      "avg / total       0.51      0.50      0.50    638010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate FS4_new P10 HMM\n",
    "FS4new_pred10 = model_p10.predict(X_test, lengths_test)\n",
    "scores = model_p10.predict_proba(X_test.values, lengths_test)\n",
    "scores = scores[: ,1]\n",
    "\n",
    "print(\"Accuracy for FS4_new P10 HMM:\",accuracy_score(y_test, FS4new_pred10))\n",
    "print(\"AUC:\",roc_auc_score(y_test, scores)) \n",
    "print(metrics.confusion_matrix(y_test, FS4new_pred10))\n",
    "print(metrics.classification_report(y_test, FS4new_pred10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
